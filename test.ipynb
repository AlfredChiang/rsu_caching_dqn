{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from myenv import MyEnv\n",
    "import numpy as np\n",
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from DQN_brain import DeepQNetwork\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    densities = [0.002]  # 0.002, 0.006, 0.01\n",
    "    smooth = 3\n",
    "    plot_DQN_rewards = []\n",
    "    plot_DQN_serveratio = []\n",
    "    plot_DQN_incentives = []\n",
    "\n",
    "    plot_RANDOM_rewards = []\n",
    "    plot_RANDOM_serveratio = []\n",
    "    plot_RANDOM_incentives = []\n",
    "\n",
    "    plot_POPULAR_rewards = []\n",
    "    plot_POPULAR_serveratio = []\n",
    "    plot_POPULAR_incentives = []\n",
    "\n",
    "    plot_MIN_rewards = []\n",
    "    plot_MIN_serveratio = []\n",
    "    plot_MIN_incentives = []\n",
    "\n",
    "    RL = False\n",
    "    for density in densities:\n",
    "        DQN_rewards = []\n",
    "        DQN_serveratio = []\n",
    "        DQN_incentives = []\n",
    "\n",
    "        RANDOM_rewards = []\n",
    "        RANDOM_serveratio = []\n",
    "        RANDOM_incentives = []\n",
    "\n",
    "        POPULAR_rewards = []\n",
    "        POPULAR_serveratio = []\n",
    "        POPULAR_incentives = []\n",
    "\n",
    "        MIN_rewards = []\n",
    "        MIN_serveratio = []\n",
    "        MIN_incentives = []\n",
    "\n",
    "        rewards = []\n",
    "\n",
    "        # Number of trials (episodes)\n",
    "        no_episodes = 50\n",
    "\n",
    "        stats = plotting.EpisodeStats(\n",
    "            episode_lengths=np.zeros(no_episodes),\n",
    "            episode_rewards=np.zeros(no_episodes))\n",
    "\n",
    "        T = 2000\n",
    "        number_of_contents = 10\n",
    "        myenv = MyEnv(density=density, T=T, number_of_contents=number_of_contents)\n",
    "\n",
    "        if (RL == False):\n",
    "            RL = DeepQNetwork(myenv.no_actions, myenv.observation_length,\n",
    "                              learning_rate=0.001,\n",
    "                              reward_decay=0.9,\n",
    "                              e_greedy=0.9,\n",
    "                              replace_target_iter=5000,\n",
    "                              memory_size=2000,\n",
    "                              batch_size=220\n",
    "                              # output_graph=True\n",
    "                              )\n",
    "\n",
    "        print(\"No. vehicles:\" + str(myenv.number_of_vehicles));\n",
    "\n",
    "        for e in range(no_episodes):\n",
    "\n",
    "            myenv = MyEnv(density=density, T=T, number_of_contents=number_of_contents)\n",
    "            myenv.episode = e\n",
    "            myenv.no_episodes = no_episodes\n",
    "            # Reset the envirounment\n",
    "            observation = myenv.reset();\n",
    "            #print(\"eeee\", e)\n",
    "            # if (e == no_episodes - 1):\n",
    "            #    print(myenv.contents_sizes)\n",
    "            for t in range(T - 1):\n",
    "                #print(t)\n",
    "                # take action\n",
    "                # RL choose action based on observation\n",
    "                action = RL.choose_action(np.array(observation))\n",
    "\n",
    "                if (e == no_episodes - 1):\n",
    "                    print(\"Observation:\", observation)\n",
    "                    print(\"Action:\", action)\n",
    "\n",
    "                # process the action and get the new observation\n",
    "                reward, observation_, Done = myenv.step(action)\n",
    "\n",
    "                if (e == no_episodes - 1):\n",
    "                    print(\"RSU cache:\", myenv.RSU_cache)\n",
    "                    print(\"Fetched:\", myenv.fetched)\n",
    "                    print(\"Served:\", myenv.served)\n",
    "                    print(\"Reward:\", reward)\n",
    "                    print(\"Total Reward:\", myenv.total_reward)\n",
    "                    print(\"\\n****\")\n",
    "\n",
    "                # Next observation is terminal\n",
    "                # if (t == myenv.T - 1):\n",
    "                # observation_ = 'terminal'\n",
    "\n",
    "                # RL learn from this transition\n",
    "                RL.store_transition(np.array(observation), action, reward, np.array(observation_))\n",
    "\n",
    "                if (t % 5 == 0):\n",
    "                    RL.learn()\n",
    "                # print(observation)\n",
    "                # print(action)\n",
    "                # print(observation_)\n",
    "                # print(\"_____________________________________________\")\n",
    "\n",
    "                # swap current and next observation\n",
    "                observation = observation_\n",
    "\n",
    "                stats.episode_rewards[e] += reward\n",
    "                stats.episode_lengths[e] = t\n",
    "\n",
    "            DQN_rewards.append(myenv.total_reward)\n",
    "            DQN_serveratio.append(myenv.total_download / myenv.total_request_amount)\n",
    "            DQN_incentives.append(myenv.total_energy)\n",
    "\n",
    "            if (e % 30 == 0):\n",
    "                print(\"Episode:\", str(e));\n",
    "\n",
    "            if (e == no_episodes - 1):\n",
    "                print(\"availables:\", myenv.available)\n",
    "                print(\"requests:\", myenv.requests)\n",
    "                print(\"content sizes:\", myenv.contents_sizes)\n",
    "                print(\"total reward:\", myenv.total_reward)\n",
    "                print(\"total incentives:\", myenv.total_energy)\n",
    "                print(\"total download:\", myenv.total_download)\n",
    "            #     ### RANDOM starts\n",
    "            #     myenv.reset()\n",
    "            #     myenv.i = 0\n",
    "            #     taken = 0\n",
    "            #     for t in range(T):\n",
    "            #         reward, observation_, Done = myenv.step(random.randint(0,myenv.no_actions-1))\n",
    "            #     RANDOM_rewards.append(myenv.total_reward)\n",
    "            #     RANDOM_serveratio.append(myenv.total_download/myenv.total_request_amount)\n",
    "            #     RANDOM_incentives.append(myenv.total_energy)\n",
    "            #\n",
    "            ### POPULAR starts\n",
    "\n",
    "            myenv.reset()\n",
    "            commit = -1\n",
    "\n",
    "            for t in range(T - 1):\n",
    "                action = myenv.no_actions - 1\n",
    "                max_index = 0\n",
    "                for p in range(myenv.no_actions):\n",
    "                    if (observation_[p * 4] > observation_[max_index * 4]):\n",
    "                        max_index = p\n",
    "\n",
    "                prefix = ((myenv.no_actions - 1) * 4) - 1\n",
    "\n",
    "                cached_min_index = 0\n",
    "\n",
    "                for cached in range(myenv.max_cached_contents):\n",
    "                    if (observation_[prefix + (cached * 2)] < observation_[prefix + cached_min_index]):\n",
    "                        cached_min_index = cached\n",
    "\n",
    "                if (observation_[max_index * 4] > observation_[prefix + cached_min_index] or myenv.RSU_cache_free() >=\n",
    "                        observation_[(max_index * 4) + 2]):\n",
    "                    action = max_index\n",
    "\n",
    "                reward, observation_, Done = myenv.step(action)\n",
    "            #     if (e == no_episodes - 1):\n",
    "            #         print(\"POPULAR RSU cache:\", myenv.RSU_cache)\n",
    "            #\n",
    "            if (e == no_episodes - 1):\n",
    "                print(\"Populer total reward:\", myenv.total_reward)\n",
    "                print(\"Populer total incentives:\", myenv.total_energy)\n",
    "                print(\"Populer total download:\", myenv.total_download)\n",
    "\n",
    "            POPULAR_rewards.append(myenv.total_reward)\n",
    "            POPULAR_serveratio.append(myenv.total_download / myenv.total_request_amount)\n",
    "            POPULAR_incentives.append(myenv.total_energy)\n",
    "        #\n",
    "        #     ### MIN starts\n",
    "        #     myenv.reset()\n",
    "        #     myenv.i = 0\n",
    "        #     taken = 0\n",
    "        #     commit = -1\n",
    "        #     for t in range(T):\n",
    "        #         action = myenv.no_actions-1\n",
    "        #         max_index = 0\n",
    "        #         for p in range(myenv.no_actions):\n",
    "        #             if(observation_[1+p*3] < observation_[1+max_index * 3]):\n",
    "        #                 max_index = p\n",
    "        #\n",
    "        #         reward, observation_, Done = myenv.step(max_index)\n",
    "        #         #if (e == no_episodes - 1):\n",
    "        #             #print(\"POPULAR RSU cache:\", myenv.RSU_cache)\n",
    "        #\n",
    "        #     MIN_rewards.append(myenv.total_reward)\n",
    "        #     MIN_serveratio.append(myenv.total_download / myenv.total_request_amount)\n",
    "        #     MIN_incentives.append(myenv.total_energy)\n",
    "        #\n",
    "        # plot_DQN_rewards.append(sum(DQN_rewards)/no_episodes)\n",
    "        # plot_DQN_serveratio.append(sum(DQN_serveratio)/no_episodes)\n",
    "        # plot_DQN_incentives.append(sum(DQN_incentives)/no_episodes)\n",
    "        #\n",
    "        # plot_RANDOM_rewards.append(sum(RANDOM_rewards) / no_episodes)\n",
    "        # plot_RANDOM_serveratio.append(sum(RANDOM_serveratio)/no_episodes)\n",
    "        # plot_RANDOM_incentives.append(sum(RANDOM_incentives)/no_episodes)\n",
    "        #\n",
    "        plot_POPULAR_rewards.append(sum(POPULAR_rewards) / no_episodes)\n",
    "        plot_POPULAR_serveratio.append(sum(POPULAR_serveratio) / no_episodes)\n",
    "        plot_POPULAR_incentives.append(sum(POPULAR_incentives) / no_episodes)\n",
    "        #\n",
    "        # plot_MIN_rewards.append(sum(MIN_rewards) / no_episodes)\n",
    "        # plot_MIN_serveratio.append(sum(MIN_serveratio) / no_episodes)\n",
    "        # plot_MIN_incentives.append(sum(MIN_incentives) / no_episodes)\n",
    "\n",
    "    print(\"DQN Reward:\" + str(sum(DQN_rewards) / no_episodes) + \" Serve Ratio:\" + str(sum(DQN_serveratio) / no_episodes)\n",
    "          + \" Incentives:\" + str(sum(DQN_incentives) / no_episodes))\n",
    "    # print(\"RANDOM Reward:\" + str(sum(RANDOM_rewards) / no_episodes) + \" Serve Ratio:\"+str(sum(RANDOM_serveratio)/no_episodes)\n",
    "    #       + \" Incentives:\"+str(sum(RANDOM_incentives)/no_episodes))\n",
    "    print(\"POPULAR Reward:\" + str(sum(POPULAR_rewards) / no_episodes) + \" Serve Ratio:\" + str(\n",
    "        sum(POPULAR_serveratio) / no_episodes)\n",
    "          + \" Incentives:\" + str(sum(POPULAR_incentives) / no_episodes))\n",
    "    # print(\"MIN Reward:\" + str(sum(MIN_rewards) / no_episodes) + \" Serve Ratio:\"+str(sum(MIN_serveratio)/no_episodes)\n",
    "    #       + \" Incentives:\"+str(sum(MIN_incentives)/no_episodes))\n",
    "\n",
    "    # plotting.plot_episode_stats(stats, smoothing_window=200)\n",
    "\n",
    "    rewards_smoothed = pd.Series(DQN_rewards).rolling(smooth, min_periods=smooth).mean()\n",
    "    plt.plot(rewards_smoothed, linewidth=5, color='g', label='DQN')\n",
    "\n",
    "    rewards_smoothed = pd.Series(RANDOM_rewards).rolling(smooth, min_periods=smooth).mean()\n",
    "    plt.plot(rewards_smoothed, linewidth=5, color='r', label='RANDOM')\n",
    "\n",
    "    rewards_smoothed = pd.Series(POPULAR_rewards).rolling(smooth, min_periods=smooth).mean()\n",
    "    plt.plot(rewards_smoothed, linewidth=5, color='b', label='POPULAR')\n",
    "\n",
    "    rewards_smoothed = pd.Series(MIN_rewards).rolling(smooth, min_periods=smooth).mean()\n",
    "    plt.plot(rewards_smoothed, linewidth=5, color='y', label='MIN')\n",
    "\n",
    "    plt.ylabel('Total Revenue (X100 Units)', fontsize=40)\n",
    "    plt.xlabel('Episodes', fontsize=40)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xticks(fontsize=35)\n",
    "    plt.yticks(fontsize=35)\n",
    "    plt.legend(prop={'size': 33})\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    f = open(\"result.txt\", \"a\")\n",
    "    f.write(\"r1=\" + str(plot_DQN_rewards) + \"\\n\")\n",
    "    f.write(\"c1=\" + str(plot_DQN_serveratio) + \"\\n\")\n",
    "    f.write(\"s1=\" + str(plot_DQN_incentives) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"r2=\" + str(plot_RANDOM_rewards) + \"\\n\")\n",
    "    f.write(\"c2=\" + str(plot_RANDOM_serveratio) + \"\\n\")\n",
    "    f.write(\"s2=\" + str(plot_RANDOM_incentives) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"r3=\" + str(plot_POPULAR_rewards) + \"\\n\")\n",
    "    f.write(\"c3=\" + str(plot_POPULAR_serveratio) + \"\\n\")\n",
    "    f.write(\"s3=\" + str(plot_POPULAR_incentives) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"r4=\" + str(plot_MIN_rewards) + \"\\n\")\n",
    "    f.write(\"c4=\" + str(plot_MIN_serveratio) + \"\\n\")\n",
    "    f.write(\"s4=\" + str(plot_MIN_incentives) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"________________________END EXPERMINT____________________________\" + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
